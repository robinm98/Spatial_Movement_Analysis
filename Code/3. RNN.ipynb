{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_33341/594484751.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../Data/merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wolf B042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LR: 1e-05, Nodes: 4, Layers: 3 has MSE: 0.3267538547515869\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 5 has MSE: 420.4384460449219\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 7 has MSE: 0.27821654081344604\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 9 has MSE: 22.64444923400879\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 3 has MSE: 0.3163612484931946\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 5 has MSE: 9.414641380310059\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 7 has MSE: 0.3277354836463928\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 9 has MSE: 0.11875331401824951\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 3 has MSE: 0.09864247590303421\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 5 has MSE: 0.008320157416164875\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 7 has MSE: 0.008935135789215565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_33341/594484751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# Fit the model with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 history = model.fit(\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Using 20% of the training data for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Data/merged_data.csv')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Define grid search parameters\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "nodes = [4, 16, 64, 256]\n",
    "layers = [3, 5, 7, 9]\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "time_steps = 20  # Number of time steps in the RNN window\n",
    "\n",
    "# Prepare a dictionary to store the results for each wolf\n",
    "results = {}\n",
    "\n",
    "# Function to create overlapping windows\n",
    "def create_overlapping_windows(data, time_steps):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        Y.append(data.iloc[i+time_steps].values)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Process each wolf's data\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "    print(f\"Processing wolf {wolf_id}\")\n",
    "    \n",
    "    # Create overlapping windows\n",
    "    X, y = create_overlapping_windows(group[['location-lat', 'location-long']], time_steps)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler_X.fit_transform(X.reshape(-1, 2)).reshape(X.shape)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    n_train = int(0.8 * len(X_scaled))\n",
    "    X_train_scaled, y_train_scaled = X_scaled[:n_train], y_scaled[:n_train]\n",
    "    X_test_scaled, y_test_scaled = X_scaled[n_train:], y_scaled[n_train:]\n",
    "\n",
    "    # Initialize the best score to a high value\n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    # Grid search for hyperparameters\n",
    "    for lr in learning_rates:\n",
    "        for node in nodes:\n",
    "            for layer in layers:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                \n",
    "                model.add(SimpleRNN(node, return_sequences=True, input_shape=(time_steps, 2), activation='relu')) # First RNN layer\n",
    "                \n",
    "                for _ in range(layer - 2): # -2 because first and last layers are already added\n",
    "                    model.add(SimpleRNN(node, return_sequences=True, activation='relu'))\n",
    "                model.add(SimpleRNN(node, activation='relu'))  # Last RNN layer\n",
    "                \n",
    "                model.add(Dense(2, activation='linear'))  # Output layer\n",
    "                \n",
    "                # Compile the model\n",
    "                optimizer = Adam(learning_rate=lr)\n",
    "                model.compile(loss='mse', optimizer=optimizer)\n",
    "                \n",
    "                # Fit the model with early stopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                history = model.fit(\n",
    "                    X_train_scaled, y_train_scaled, \n",
    "                    validation_split=0.2,  # Using 20% of the training data for validation\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate the model on the test data\n",
    "                score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "                print(f\"Model with LR: {lr}, Nodes: {node}, Layers: {layer} has MSE: {score}\")\n",
    "\n",
    "                # Update best model if the score improved\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'lr': lr, 'nodes': node, 'layers': layer}\n",
    "                    best_model = model\n",
    "\n",
    "    # Forecast future values using the last available input from training\n",
    "    current_input = X_train_scaled[-1].reshape(1, time_steps, 2)\n",
    "    predictions = []\n",
    "    for _ in range(len(y_test_scaled)):\n",
    "        next_prediction = best_model.predict(current_input)\n",
    "        predictions.append(next_prediction[0])\n",
    "        current_input = np.roll(current_input, -1, axis=1)\n",
    "        current_input[0, -1, :] = next_prediction[0]\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    predictions_scaled_back = scaler_y.inverse_transform(predictions)\n",
    "    mse = mean_squared_error(scaler_y.inverse_transform(y_test_scaled), predictions_scaled_back)\n",
    "    \n",
    "    results[wolf_id] = {\n",
    "        'mse': mse,\n",
    "        'predictions': predictions_scaled_back,\n",
    "        'actual': scaler_y.inverse_transform(y_test_scaled),\n",
    "        'best_params': best_params\n",
    "    }\n",
    "    \n",
    "    # Save the best model for each wolf\n",
    "    best_model.save(f'../Code/RNN model/best_model_for_wolf_{wolf_id}.keras')\n",
    "\n",
    "# Output the results for each wolf\n",
    "for wolf_id, res in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}, MSE: {res['mse']}, Best Params: {res['best_params']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
