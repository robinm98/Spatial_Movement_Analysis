{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_33081/3679044923.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../Data/merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wolf B042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LR: 1e-05, Nodes: 4, Layers: 3 has MSE: 0.2781844139099121\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 5 has MSE: 0.29451772570610046\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 7 has MSE: 0.33325302600860596\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 9 has MSE: 0.3129555284976959\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 3 has MSE: 0.07164628803730011\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 5 has MSE: 0.4484039843082428\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 7 has MSE: 0.37608006596565247\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 9 has MSE: 0.3010789453983307\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 3 has MSE: 0.07516578584909439\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 5 has MSE: 0.06813401728868484\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 7 has MSE: 0.21391110122203827\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 9 has MSE: 0.21845434606075287\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 3 has MSE: 0.010170698165893555\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 5 has MSE: 0.006318354979157448\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 7 has MSE: 0.00771325221285224\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 9 has MSE: 0.00814784225076437\n",
      "Model with LR: 0.0001, Nodes: 4, Layers: 3 has MSE: 0.2484007477760315\n",
      "Model with LR: 0.0001, Nodes: 4, Layers: 5 has MSE: 0.3011886775493622\n",
      "Model with LR: 0.0001, Nodes: 4, Layers: 7 has MSE: 0.2901543974876404\n",
      "Model with LR: 0.0001, Nodes: 4, Layers: 9 has MSE: 0.15156219899654388\n",
      "Model with LR: 0.0001, Nodes: 16, Layers: 3 has MSE: 0.06150248646736145\n",
      "Model with LR: 0.0001, Nodes: 16, Layers: 5 has MSE: 0.00839910563081503\n",
      "Model with LR: 0.0001, Nodes: 16, Layers: 7 has MSE: 0.030922871083021164\n",
      "Model with LR: 0.0001, Nodes: 16, Layers: 9 has MSE: 0.0076748887076973915\n",
      "Model with LR: 0.0001, Nodes: 64, Layers: 3 has MSE: 0.0070440927520394325\n",
      "Model with LR: 0.0001, Nodes: 64, Layers: 5 has MSE: 0.006771461572498083\n",
      "Model with LR: 0.0001, Nodes: 64, Layers: 7 has MSE: 0.006532987114042044\n",
      "Model with LR: 0.0001, Nodes: 64, Layers: 9 has MSE: 0.007477625273168087\n",
      "Model with LR: 0.0001, Nodes: 256, Layers: 3 has MSE: 0.005143746268004179\n",
      "Model with LR: 0.0001, Nodes: 256, Layers: 5 has MSE: 0.00520997354760766\n",
      "Model with LR: 0.0001, Nodes: 256, Layers: 7 has MSE: 0.005004793405532837\n",
      "Model with LR: 0.0001, Nodes: 256, Layers: 9 has MSE: 0.005604355130344629\n",
      "Model with LR: 0.001, Nodes: 4, Layers: 3 has MSE: 0.008536345325410366\n",
      "Model with LR: 0.001, Nodes: 4, Layers: 5 has MSE: 0.12217330187559128\n",
      "Model with LR: 0.001, Nodes: 4, Layers: 7 has MSE: 0.011032969690859318\n",
      "Model with LR: 0.001, Nodes: 4, Layers: 9 has MSE: 0.008677671663463116\n",
      "Model with LR: 0.001, Nodes: 16, Layers: 3 has MSE: 0.007705955300480127\n",
      "Model with LR: 0.001, Nodes: 16, Layers: 5 has MSE: 0.007734771817922592\n",
      "Model with LR: 0.001, Nodes: 16, Layers: 7 has MSE: 0.007181847933679819\n",
      "Model with LR: 0.001, Nodes: 16, Layers: 9 has MSE: 0.007994886487722397\n",
      "Model with LR: 0.001, Nodes: 64, Layers: 3 has MSE: 0.00497118616476655\n",
      "Model with LR: 0.001, Nodes: 64, Layers: 5 has MSE: 0.006114304065704346\n",
      "Model with LR: 0.001, Nodes: 64, Layers: 7 has MSE: 0.007563459686934948\n",
      "Model with LR: 0.001, Nodes: 64, Layers: 9 has MSE: 0.007005899678915739\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Data/merged_data.csv')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Define grid search parameters\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "nodes = [4, 16, 64, 256]\n",
    "layers = [3, 5, 7, 9]\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "n_lags = 20 # Number of lagged observations to use\n",
    "\n",
    "# Prepare a dictionary to store the results for each wolf\n",
    "results = {}\n",
    "\n",
    "# Function to create lagged features\n",
    "def create_lagged_features(data, n_lags):\n",
    "    X, Y = [], []\n",
    "    for i in range(n_lags, len(data)):\n",
    "        X.append(data.iloc[i-n_lags:i].values.flatten())\n",
    "        Y.append(data.iloc[i].values)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Process each wolf's data\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "    print(f\"Processing wolf {wolf_id}\")\n",
    "    \n",
    "    X, y = create_lagged_features(group[['location-lat', 'location-long']], n_lags)\n",
    "    \n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Split the data\n",
    "    n_train = int(0.8 * len(X_scaled))\n",
    "    X_train_scaled, y_train_scaled = X_scaled[:n_train], y_scaled[:n_train]\n",
    "    X_test_scaled, y_test_scaled = X_scaled[n_train:], y_scaled[n_train:]\n",
    "\n",
    "    # Initialize the best score to some high value\n",
    "    best_score = np.inf\n",
    "\n",
    "    # Grid search\n",
    "    for lr in learning_rates:\n",
    "        for node in nodes:\n",
    "            for layer in layers:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Dense(node, input_dim=n_lags*2, activation='relu'))  # First layer\n",
    "                \n",
    "                for _ in range(layer-1):\n",
    "                    model.add(Dense(node, activation='relu'))\n",
    "\n",
    "                model.add(Dense(2, activation='linear'))  # Output layer\n",
    "                \n",
    "                # Compile the model\n",
    "                optimizer = Adam(learning_rate=lr)\n",
    "                model.compile(loss='mse', optimizer=optimizer)\n",
    "                \n",
    "                # Train the model with EarlyStopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                history = model.fit(\n",
    "                    X_train_scaled, y_train_scaled, \n",
    "                    validation_split=0.2,  # Using 20% of the training data for validation\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate the model\n",
    "                score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "                print(f\"Model with LR: {lr}, Nodes: {node}, Layers: {layer} has MSE: {score}\")\n",
    "\n",
    "                # Update best model if improved\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'lr': lr, 'nodes': node, 'layers': layer}\n",
    "                    best_model = model\n",
    "\n",
    "    # Forecast future values using the last available input from training\n",
    "    current_input = X_train_scaled[-1:].reshape(1, -1)  # Reshape for single prediction input\n",
    "    predictions = []\n",
    "    for _ in range(len(X_test_scaled)):\n",
    "        next_prediction = best_model.predict(current_input)\n",
    "        predictions.append(next_prediction)\n",
    "        current_input = np.roll(current_input, -2)\n",
    "        current_input[0, -2:] = next_prediction\n",
    "\n",
    "\n",
    "    # Transform predictions back to original scale\n",
    "    predictions_scaled_back = scaler_y.inverse_transform(np.vstack(predictions))\n",
    "    mse = mean_squared_error(y_test_scaled, predictions_scaled_back)\n",
    "\n",
    "    results[wolf_id] = {\n",
    "        'mse': mse,\n",
    "        'predictions': predictions_scaled_back,\n",
    "        'actual': scaler_y.inverse_transform(y_test_scaled),\n",
    "        'best_params': best_params\n",
    "    }\n",
    "    \n",
    "    # Save the best model for each wolf in the Keras format\n",
    "    best_model.save('../Code/FNN model/best_model_for_wolf_{wolf_id}.keras') \n",
    "\n",
    "# Output the results for each wolf\n",
    "for wolf_id, res in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}, MSE: {res['mse']}, Best Params: {res['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving / loading back the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "\n",
    "    # Convert results dictionary to a DataFrame for easier CSV saving\n",
    "    results_df = pd.DataFrame([results[wolf_id]])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_csv_path = f'../Code/FNN model/results_{wolf_id}.csv'\n",
    "    results_df.to_csv(results_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unique_wolf_ids = data['individual-id'].unique()\n",
    "\n",
    "for wolf_id in unique_wolf_ids:\n",
    "    # Define paths\n",
    "    results_csv_path = f'../Code/FNN model/results_{wolf_id}.csv'\n",
    "\n",
    "    # Load results from CSV\n",
    "    loaded_results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "    # Convert DataFrame back to dictionary \n",
    "    loaded_results[wolf_id] = loaded_results_df.to_dict(orient='records')[0]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a map for a single wolf\n",
    "def create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id):\n",
    "    # Starting point for the map\n",
    "    start_lat = wolf_data['location-lat'].iloc[0]\n",
    "    start_long = wolf_data['location-long'].iloc[0]\n",
    "    wolf_map = folium.Map(location=[start_lat, start_long], zoom_start=8)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(wolf_data) * 0.8)\n",
    "    train_data = wolf_data.iloc[:split_index]\n",
    "    test_data = wolf_data.iloc[split_index:]\n",
    "\n",
    "    # Define the HTML code for a purple cross\n",
    "    html_purple_cross = '''\n",
    "    <div style=\"position: relative; width: 8px; height: 8px;\">\n",
    "        <div style=\"position: absolute; top: 50%; left: 0; transform: translate(0%, -50%); width: 100%; height: 2px; background-color: purple;\"></div>\n",
    "        <div style=\"position: absolute; top: 0; left: 50%; transform: translate(-50%, 0%); width: 2px; height: 100%; background-color: purple;\"></div>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "    # Add training data points\n",
    "    train_points = [(row['location-lat'], row['location-long']) for idx, row in train_data.iterrows()]\n",
    "    folium.PolyLine(train_points, color=\"blue\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in train_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            popup='Train'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Add test data points\n",
    "    test_points = [(row['location-lat'], row['location-long']) for idx, row in test_data.iterrows()]\n",
    "    folium.PolyLine(test_points, color=\"green\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in test_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='green',\n",
    "            popup='Test'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Mark the first test observation with a purple cross\n",
    "    folium.Marker(\n",
    "        location=test_points[0],\n",
    "        icon=folium.DivIcon(html=html_purple_cross),\n",
    "        popup='First Test Observation'\n",
    "    ).add_to(wolf_map)\n",
    "\n",
    "    # Add prediction points and draw lines between them\n",
    "    prediction_points = list(zip(test_preds_lat, test_preds_long))\n",
    "    folium.PolyLine(prediction_points, color=\"red\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for idx, point in enumerate(prediction_points):\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            popup=f'Predicted: {idx}'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Save the map in a specific directory for FNN results\n",
    "    wolf_map.save(f'../Visualisation/FNN/wolf_{wolf_id}_map.html')\n",
    "\n",
    "# Iterate through each wolf and create a map\n",
    "for wolf_id, info in results.items():\n",
    "    test_preds_lat = info['predictions'][:, 0]  \n",
    "    test_preds_long = info['predictions'][:, 1]\n",
    "    wolf_data = data[data['individual-id'] == wolf_id]\n",
    "    create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN accuracy for each wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf ID: B042\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0161\n",
      "RMSE Latitude: 0.0290\n",
      "MAE Longitude: 0.1038\n",
      "RMSE Longitude: 0.1133\n",
      "Mean Haversine Distance: 7.69 km\n",
      "Median Haversine Distance: 7.14 km\n",
      "----------\n",
      "Wolf ID: B045\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0336\n",
      "RMSE Latitude: 0.0544\n",
      "MAE Longitude: 0.0877\n",
      "RMSE Longitude: 0.1006\n",
      "Mean Haversine Distance: 8.11 km\n",
      "Median Haversine Distance: 7.25 km\n",
      "----------\n",
      "Wolf ID: B065\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0865\n",
      "RMSE Latitude: 0.0971\n",
      "MAE Longitude: 0.4116\n",
      "RMSE Longitude: 0.4212\n",
      "Mean Haversine Distance: 30.38 km\n",
      "Median Haversine Distance: 30.24 km\n",
      "----------\n",
      "Wolf ID: B077\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0871\n",
      "RMSE Latitude: 0.0978\n",
      "MAE Longitude: 0.2944\n",
      "RMSE Longitude: 0.3175\n",
      "Mean Haversine Distance: 22.68 km\n",
      "Median Haversine Distance: 26.28 km\n",
      "----------\n",
      "Wolf ID: B078\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.5988\n",
      "RMSE Latitude: 0.7137\n",
      "MAE Longitude: 2.0278\n",
      "RMSE Longitude: 2.3237\n",
      "Mean Haversine Distance: 153.95 km\n",
      "Median Haversine Distance: 155.44 km\n",
      "----------\n",
      "Wolf ID: B079\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.1645\n",
      "RMSE Latitude: 0.1692\n",
      "MAE Longitude: 0.0370\n",
      "RMSE Longitude: 0.0413\n",
      "Mean Haversine Distance: 18.51 km\n",
      "Median Haversine Distance: 19.61 km\n",
      "----------\n",
      "Wolf ID: B080\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0794\n",
      "RMSE Latitude: 0.0961\n",
      "MAE Longitude: 0.1284\n",
      "RMSE Longitude: 0.1604\n",
      "Mean Haversine Distance: 12.76 km\n",
      "Median Haversine Distance: 9.95 km\n",
      "----------\n",
      "Wolf ID: B081\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0513\n",
      "RMSE Latitude: 0.0746\n",
      "MAE Longitude: 0.4011\n",
      "RMSE Longitude: 0.4377\n",
      "Mean Haversine Distance: 29.40 km\n",
      "Median Haversine Distance: 28.86 km\n",
      "----------\n",
      "Wolf ID: B082\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.4420\n",
      "RMSE Latitude: 0.4535\n",
      "MAE Longitude: 0.6310\n",
      "RMSE Longitude: 0.6539\n",
      "Mean Haversine Distance: 65.85 km\n",
      "Median Haversine Distance: 72.07 km\n",
      "----------\n",
      "Wolf ID: B083\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.1477\n",
      "RMSE Latitude: 0.1633\n",
      "MAE Longitude: 0.0495\n",
      "RMSE Longitude: 0.0641\n",
      "Mean Haversine Distance: 17.00 km\n",
      "Median Haversine Distance: 17.24 km\n",
      "----------\n",
      "Wolf ID: B084\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0405\n",
      "RMSE Latitude: 0.0575\n",
      "MAE Longitude: 0.1617\n",
      "RMSE Longitude: 0.2272\n",
      "Mean Haversine Distance: 12.99 km\n",
      "Median Haversine Distance: 7.83 km\n",
      "----------\n",
      "Wolf ID: B085\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.1186\n",
      "RMSE Latitude: 0.1302\n",
      "MAE Longitude: 0.1434\n",
      "RMSE Longitude: 0.1633\n",
      "Mean Haversine Distance: 17.02 km\n",
      "Median Haversine Distance: 16.07 km\n",
      "----------\n",
      "Wolf ID: B086\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.2658\n",
      "RMSE Latitude: 0.2727\n",
      "MAE Longitude: 0.1386\n",
      "RMSE Longitude: 0.1542\n",
      "Mean Haversine Distance: 31.35 km\n",
      "Median Haversine Distance: 32.72 km\n",
      "----------\n",
      "Wolf ID: B087\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.1476\n",
      "RMSE Latitude: 0.1677\n",
      "MAE Longitude: 0.4569\n",
      "RMSE Longitude: 0.4942\n",
      "Mean Haversine Distance: 37.35 km\n",
      "Median Haversine Distance: 34.19 km\n",
      "----------\n",
      "Wolf ID: JW01\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.0615\n",
      "RMSE Latitude: 0.0787\n",
      "MAE Longitude: 0.7503\n",
      "RMSE Longitude: 0.7521\n",
      "Mean Haversine Distance: 53.57 km\n",
      "Median Haversine Distance: 53.89 km\n",
      "----------\n",
      "Wolf ID: JW02\n",
      "Best Model: Learning Rate: 0.001, Nodes: 64, Layers: 5\n",
      "MAE Latitude: 0.3507\n",
      "RMSE Latitude: 0.3947\n",
      "MAE Longitude: 0.3149\n",
      "RMSE Longitude: 0.3521\n",
      "Mean Haversine Distance: 44.88 km\n",
      "Median Haversine Distance: 60.10 km\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points \n",
    "    on the Earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers. Adjust if using a different radius\n",
    "    return c * r\n",
    "\n",
    "for wolf_id, res in results.items():\n",
    "    test_data = res['actual']\n",
    "    predictions = res['predictions']\n",
    "    \n",
    "    # Extract latitude and longitude from predictions and actual data\n",
    "    lat_preds, lon_preds = predictions[:, 0], predictions[:, 1]\n",
    "    lat_actual, lon_actual = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "    # Calculate the Haversine distance for each prediction\n",
    "    distances = [haversine(lon_p, lat_p, lon_a, lat_a) for lon_p, lat_p, lon_a, lat_a in zip(lon_preds, lat_preds, lon_actual, lat_actual)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae_lat = mean_absolute_error(lat_actual, lat_preds)\n",
    "    rmse_lat = sqrt(mean_squared_error(lat_actual, lat_preds))\n",
    "    mae_lon = mean_absolute_error(lon_actual, lon_preds)\n",
    "    rmse_lon = sqrt(mean_squared_error(lon_actual, lon_preds))\n",
    "    \n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    \n",
    "    # Store extended metrics in the results dictionary\n",
    "    res.update({\n",
    "        'MAE_Latitude': mae_lat,\n",
    "        'RMSE_Latitude': rmse_lat,\n",
    "        'MAE_Longitude': mae_lon,\n",
    "        'RMSE_Longitude': rmse_lon,\n",
    "        'Mean_Haversine_Distance': mean_distance,\n",
    "        'Median_Haversine_Distance': median_distance\n",
    "    })\n",
    "\n",
    "# Print the metrics for each wolf\n",
    "for wolf_id, info in results.items():\n",
    "    print(f\"FNN - Wolf ID: {wolf_id}\")\n",
    "    print(f\"Best Model: Learning Rate: {res['best_params']['lr']}, Nodes: {res['best_params']['nodes']}, Layers: {res['best_params']['layers']}\")\n",
    "    print(f\"MAE Latitude: {info['MAE_Latitude']:.4f}\")\n",
    "    print(f\"RMSE Latitude: {info['RMSE_Latitude']:.4f}\")\n",
    "    print(f\"MAE Longitude: {info['MAE_Longitude']:.4f}\")\n",
    "    print(f\"RMSE Longitude: {info['RMSE_Longitude']:.4f}\")\n",
    "    print(f\"Mean Haversine Distance: {info['Mean_Haversine_Distance']:.2f} km\")\n",
    "    print(f\"Median Haversine Distance: {info['Median_Haversine_Distance']:.2f} km\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics for All Wolves:\n",
      "Average MAE Latitude: 0.1682\n",
      "Average RMSE Latitude: 0.1906\n",
      "Average MAE Longitude: 0.3836\n",
      "Average RMSE Longitude: 0.4236\n",
      "Average Mean Haversine Distance: 35.22 km\n",
      "Average Median Haversine Distance: 36.18 km\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to accumulate the metrics\n",
    "total_mae_lat = 0\n",
    "total_rmse_lat = 0\n",
    "total_mae_long = 0\n",
    "total_rmse_long = 0\n",
    "total_mean_haversine_distance = 0\n",
    "total_median_haversine_distance = 0\n",
    "num_wolves = len(results)\n",
    "\n",
    "# Loop over each wolf's results\n",
    "for wolf_id, info in results.items():\n",
    "    # Accumulate individual metrics\n",
    "    total_mae_lat += info['MAE_Latitude']\n",
    "    total_rmse_lat += info['RMSE_Latitude']\n",
    "    total_mae_long += info['MAE_Longitude']\n",
    "    total_rmse_long += info['RMSE_Longitude']\n",
    "    total_mean_haversine_distance += info['Mean_Haversine_Distance']\n",
    "    total_median_haversine_distance += info['Median_Haversine_Distance']\n",
    "\n",
    "# Calculate average metrics\n",
    "average_mae_lat = total_mae_lat / num_wolves\n",
    "average_rmse_lat = total_rmse_lat / num_wolves\n",
    "average_mae_long = total_mae_long / num_wolves\n",
    "average_rmse_long = total_rmse_long / num_wolves\n",
    "average_mean_haversine_distance = total_mean_haversine_distance / num_wolves\n",
    "average_median_haversine_distance = total_median_haversine_distance / num_wolves\n",
    "\n",
    "# Print average metrics\n",
    "print(\"FNN - Average Metrics for All Wolves:\")\n",
    "print(f\"Average MAE Latitude: {average_mae_lat:.4f}\")\n",
    "print(f\"Average RMSE Latitude: {average_rmse_lat:.4f}\")\n",
    "print(f\"Average MAE Longitude: {average_mae_long:.4f}\")\n",
    "print(f\"Average RMSE Longitude: {average_rmse_long:.4f}\")\n",
    "print(f\"Average Mean Haversine Distance: {average_mean_haversine_distance:.2f} km\")\n",
    "print(f\"Average Median Haversine Distance: {average_median_haversine_distance:.2f} km\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
