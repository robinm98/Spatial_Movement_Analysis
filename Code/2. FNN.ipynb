{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 11:52:23.490211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_24715/2166124846.py:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../Data/merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wolf B042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LR: 1e-05, Nodes: 4, Layers: 3 has MSE: 0.3463926315307617\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 5 has MSE: 0.24066108465194702\n",
      "Model with LR: 1e-05, Nodes: 4, Layers: 7 has MSE: 0.2847052812576294\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 3 has MSE: 0.49756303429603577\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 5 has MSE: 0.2776191532611847\n",
      "Model with LR: 1e-05, Nodes: 16, Layers: 7 has MSE: 0.32085296511650085\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 3 has MSE: 0.28341877460479736\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 5 has MSE: 0.20109692215919495\n",
      "Model with LR: 1e-05, Nodes: 64, Layers: 7 has MSE: 0.2623779773712158\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 3 has MSE: 0.03143102675676346\n",
      "Model with LR: 1e-05, Nodes: 256, Layers: 5 has MSE: 0.03531502187252045\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Data/merged_data.csv')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Define grid search parameters\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "nodes = [4, 16, 64, 256]\n",
    "layers = [3, 5, 7]\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "patience = 3\n",
    "n_lags = 3  # Number of lagged observations to use\n",
    "\n",
    "# Prepare a dictionary to store the results for each wolf\n",
    "results = {}\n",
    "\n",
    "# Function to create lagged features\n",
    "def create_lagged_features(data, n_lags):\n",
    "    X, Y = [], []\n",
    "    for i in range(n_lags, len(data)):\n",
    "        X.append(data.iloc[i-n_lags:i].values.flatten())\n",
    "        Y.append(data.iloc[i].values)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Process each wolf's data\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "    print(f\"Processing wolf {wolf_id}\")\n",
    "    \n",
    "    X, y = create_lagged_features(group[['location-lat', 'location-long']], n_lags)\n",
    "    \n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Split the data\n",
    "    n_train = int(0.8 * len(X_scaled))\n",
    "    X_train_scaled, y_train_scaled = X_scaled[:n_train], y_scaled[:n_train]\n",
    "    X_test_scaled, y_test_scaled = X_scaled[n_train:], y_scaled[n_train:]\n",
    "\n",
    "    # Initialize the best score to some high value\n",
    "    best_score = np.inf\n",
    "\n",
    "    # Grid search\n",
    "    for lr in learning_rates:\n",
    "        for node in nodes:\n",
    "            for layer in layers:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(node, input_dim=n_lags*2, activation='relu'))  # Input dimension based on lagged features\n",
    "                for _ in range(layer - 1):\n",
    "                    model.add(Dense(node, activation='relu'))\n",
    "                model.add(Dense(2, activation='linear'))  # Output layer\n",
    "                \n",
    "                # Compile the model\n",
    "                optimizer = Adam(learning_rate=lr)\n",
    "                model.compile(loss='mse', optimizer=optimizer)\n",
    "                \n",
    "                # Train the model with EarlyStopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                history = model.fit(\n",
    "                    X_train_scaled, y_train_scaled, \n",
    "                    validation_split=0.2,  # Using 20% of the training data for validation\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate the model\n",
    "                score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "                print(f\"Model with LR: {lr}, Nodes: {node}, Layers: {layer} has MSE: {score}\")\n",
    "\n",
    "                # Update best model if improved\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'lr': lr, 'nodes': node, 'layers': layer}\n",
    "                    best_model = model\n",
    "\n",
    "    # Forecast future values using the last available input from training\n",
    "    current_input = X_train_scaled[-1:].reshape(1, -1)  # Reshape for single prediction input\n",
    "    predictions = []\n",
    "    for _ in range(len(X_test_scaled)):\n",
    "        next_prediction = best_model.predict(current_input)\n",
    "        predictions.append(next_prediction)\n",
    "        current_input = np.roll(current_input, -2)\n",
    "        current_input[0, -2:] = next_prediction\n",
    "\n",
    "\n",
    "    # Transform predictions back to original scale\n",
    "    predictions_scaled_back = scaler_y.inverse_transform(np.vstack(predictions))\n",
    "    mse = mean_squared_error(y_test_scaled, predictions_scaled_back)\n",
    "\n",
    "    results[wolf_id] = {\n",
    "        'mse': mse,\n",
    "        'predictions': predictions_scaled_back,\n",
    "        'actual': scaler_y.inverse_transform(y_test_scaled),\n",
    "        'best_params': best_params\n",
    "    }\n",
    "    \n",
    "    # Save the best model for each wolf in the Keras format\n",
    "    best_model.save('../Code/FNN model/best_model_for_wolf_{wolf_id}.keras') \n",
    "\n",
    "# Output the results for each wolf\n",
    "for wolf_id, res in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}, MSE: {res['mse']}, Best Params: {res['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save results and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save results to a pickle file\n",
    "with open('../Code/FNN model/results.pickle', 'wb') as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save the scalers for X and y \n",
    "with open('../Code/FNN model/scaler_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "with open('../Code/FNN model/scaler_y.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_y, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the best models, results, and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for loading\n",
    "model_path = '../Code/FNN model/best_model_for_wolf_{wolf_id}.keras'\n",
    "results_path = '../Code/FNN model/results.pickle'\n",
    "scaler_X_path = '../Code/FNN model/scaler_X.pkl'\n",
    "scaler_y_path = '../Code/FNN model/scaler_y.pkl'\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Load the results\n",
    "with open(results_path, 'rb') as handle:\n",
    "    loaded_results = pickle.load(handle)\n",
    "\n",
    "# Load the scalers\n",
    "with open(scaler_X_path, 'rb') as f:\n",
    "    loaded_scaler_X = pickle.load(f)\n",
    "\n",
    "with open(scaler_y_path, 'rb') as f:\n",
    "    loaded_scaler_y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a map for a single wolf\n",
    "def create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id):\n",
    "    # Starting point for the map\n",
    "    start_lat = wolf_data['location-lat'].iloc[0]\n",
    "    start_long = wolf_data['location-long'].iloc[0]\n",
    "    wolf_map = folium.Map(location=[start_lat, start_long], zoom_start=8)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(wolf_data) * 0.8)\n",
    "    train_data = wolf_data.iloc[:split_index]\n",
    "    test_data = wolf_data.iloc[split_index:]\n",
    "\n",
    "    # Define the HTML code for a purple cross\n",
    "    html_purple_cross = '''\n",
    "    <div style=\"position: relative; width: 8px; height: 8px;\">\n",
    "        <div style=\"position: absolute; top: 50%; left: 0; transform: translate(0%, -50%); width: 100%; height: 2px; background-color: purple;\"></div>\n",
    "        <div style=\"position: absolute; top: 0; left: 50%; transform: translate(-50%, 0%); width: 2px; height: 100%; background-color: purple;\"></div>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "    # Add training data points\n",
    "    train_points = [(row['location-lat'], row['location-long']) for idx, row in train_data.iterrows()]\n",
    "    folium.PolyLine(train_points, color=\"blue\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in train_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            popup='Train'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Add test data points\n",
    "    test_points = [(row['location-lat'], row['location-long']) for idx, row in test_data.iterrows()]\n",
    "    folium.PolyLine(test_points, color=\"green\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in test_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='green',\n",
    "            popup='Test'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Mark the first test observation with a purple cross\n",
    "    folium.Marker(\n",
    "        location=test_points[0],\n",
    "        icon=folium.DivIcon(html=html_purple_cross),\n",
    "        popup='First Test Observation'\n",
    "    ).add_to(wolf_map)\n",
    "\n",
    "    # Add prediction points and draw lines between them\n",
    "    prediction_points = list(zip(test_preds_lat, test_preds_long))\n",
    "    folium.PolyLine(prediction_points, color=\"red\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for idx, point in enumerate(prediction_points):\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            popup=f'Predicted: {idx}'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Save the map in a specific directory for FNN results\n",
    "    wolf_map.save(f'../Visualisation/FNN/wolf_{wolf_id}_map.html')\n",
    "\n",
    "# Iterate through each wolf and create a map\n",
    "for wolf_id, info in results.items():\n",
    "    test_preds_lat = info['predictions'][:, 0]  \n",
    "    test_preds_long = info['predictions'][:, 1]\n",
    "    wolf_data = data[data['individual-id'] == wolf_id]\n",
    "    create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN accuracy for each wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'actual'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "\u001b[0;32m/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_19033/395899495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwolf_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyError\u001b[0m: 'actual'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points \n",
    "    on the Earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers. Adjust if using a different radius\n",
    "    return c * r\n",
    "\n",
    "for wolf_id, res in results.items():\n",
    "    test_data = res['actual']\n",
    "    predictions = res['predictions']\n",
    "    \n",
    "    # Extract latitude and longitude from predictions and actual data\n",
    "    lat_preds, lon_preds = predictions[:, 0], predictions[:, 1]\n",
    "    lat_actual, lon_actual = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "    # Calculate the Haversine distance for each prediction\n",
    "    distances = [haversine(lon_p, lat_p, lon_a, lat_a) for lon_p, lat_p, lon_a, lat_a in zip(lon_preds, lat_preds, lon_actual, lat_actual)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae_lat = mean_absolute_error(lat_actual, lat_preds)\n",
    "    rmse_lat = sqrt(mean_squared_error(lat_actual, lat_preds))\n",
    "    mae_lon = mean_absolute_error(lon_actual, lon_preds)\n",
    "    rmse_lon = sqrt(mean_squared_error(lon_actual, lon_preds))\n",
    "    \n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    \n",
    "    # Store extended metrics in the results dictionary\n",
    "    res.update({\n",
    "        'MAE_Latitude': mae_lat,\n",
    "        'RMSE_Latitude': rmse_lat,\n",
    "        'MAE_Longitude': mae_lon,\n",
    "        'RMSE_Longitude': rmse_lon,\n",
    "        'Mean_Haversine_Distance': mean_distance,\n",
    "        'Median_Haversine_Distance': median_distance\n",
    "    })\n",
    "\n",
    "# Print the metrics for each wolf\n",
    "for wolf_id, info in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}\")\n",
    "    print(f\"Best Model: Learning Rate: {res['best_params']['lr']}, Nodes: {res['best_params']['nodes']}, Layers: {res['best_params']['layers']}\")\n",
    "    print(f\"MAE Latitude: {info['MAE_Latitude']:.4f}\")\n",
    "    print(f\"RMSE Latitude: {info['RMSE_Latitude']:.4f}\")\n",
    "    print(f\"MAE Longitude: {info['MAE_Longitude']:.4f}\")\n",
    "    print(f\"RMSE Longitude: {info['RMSE_Longitude']:.4f}\")\n",
    "    print(f\"Mean Haversine Distance: {info['Mean_Haversine_Distance']:.2f} km\")\n",
    "    print(f\"Median Haversine Distance: {info['Median_Haversine_Distance']:.2f} km\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate the metrics\n",
    "total_mae_lat = 0\n",
    "total_rmse_lat = 0\n",
    "total_mae_long = 0\n",
    "total_rmse_long = 0\n",
    "total_mean_haversine_distance = 0\n",
    "total_median_haversine_distance = 0\n",
    "num_wolves = len(results)\n",
    "\n",
    "# Loop over each wolf's results\n",
    "for wolf_id, info in results.items():\n",
    "    # Accumulate individual metrics\n",
    "    total_mae_lat += info['MAE_Latitude']\n",
    "    total_rmse_lat += info['RMSE_Latitude']\n",
    "    total_mae_long += info['MAE_Longitude']\n",
    "    total_rmse_long += info['RMSE_Longitude']\n",
    "    total_mean_haversine_distance += info['Mean_Haversine_Distance']\n",
    "    total_median_haversine_distance += info['Median_Haversine_Distance']\n",
    "\n",
    "# Calculate average metrics\n",
    "average_mae_lat = total_mae_lat / num_wolves\n",
    "average_rmse_lat = total_rmse_lat / num_wolves\n",
    "average_mae_long = total_mae_long / num_wolves\n",
    "average_rmse_long = total_rmse_long / num_wolves\n",
    "average_mean_haversine_distance = total_mean_haversine_distance / num_wolves\n",
    "average_median_haversine_distance = total_median_haversine_distance / num_wolves\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Metrics for All Wolves:\")\n",
    "print(f\"Average MAE Latitude: {average_mae_lat:.4f}\")\n",
    "print(f\"Average RMSE Latitude: {average_rmse_lat:.4f}\")\n",
    "print(f\"Average MAE Longitude: {average_mae_long:.4f}\")\n",
    "print(f\"Average RMSE Longitude: {average_rmse_long:.4f}\")\n",
    "print(f\"Average Mean Haversine Distance: {average_mean_haversine_distance:.2f} km\")\n",
    "print(f\"Average Median Haversine Distance: {average_median_haversine_distance:.2f} km\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
