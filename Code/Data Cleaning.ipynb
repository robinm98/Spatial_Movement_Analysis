{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - Wolf data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_6519/2020764569.py:5: DtypeWarning: Columns (5,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  wolves_data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  individual-id    event-id           timestamp  location-long  location-lat  \\\n",
      "0          B042  2048103217 2002-12-20 19:02:06    -115.901616     51.656829   \n",
      "1          B042  2048103218 2002-12-20 23:01:18    -115.722444     51.686639   \n",
      "2          B042  2048103219 2002-12-21 03:02:10    -115.787129     51.679586   \n",
      "3          B042  2048103220 2002-12-21 08:02:11    -115.819219     51.681042   \n",
      "4          B042  2048103221 2002-12-21 09:01:30    -115.818068     51.680339   \n",
      "\n",
      "  animal-type Pack name  \n",
      "0        Wolf  Red Deer  \n",
      "1        Wolf  Red Deer  \n",
      "2        Wolf  Red Deer  \n",
      "3        Wolf  Red Deer  \n",
      "4        Wolf  Red Deer  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advanced Data Analysis/ADA_Project/Data/Hebblewhite Alberta-BC Wolves.csv'\n",
    "wolves_data = pd.read_csv(file_path)\n",
    "\n",
    "# Rename the column \"comments\" to \"Pack name\"\n",
    "wolves_data.rename(columns={'comments': 'Pack name'}, inplace=True)\n",
    "\n",
    "# Convert any timestamps to datetime format if they're not already\n",
    "wolves_data['timestamp'] = pd.to_datetime(wolves_data['timestamp'])\n",
    "\n",
    "# Filter the data to include only observations from 2001 to 2011\n",
    "wolves_data = wolves_data[(wolves_data['timestamp'].dt.year >= 2001) & (wolves_data['timestamp'].dt.year <= 2011)]\n",
    "\n",
    "# Add 'individual-id' and 'animal-type' columns\n",
    "wolves_data['individual-id'] = wolves_data['individual-local-identifier']\n",
    "wolves_data['animal-type'] = 'Wolf'\n",
    "\n",
    "# Filter the data to include only the specified packs\n",
    "packs = [\"Wildhorse\", \"Ranch\", \"Red Deer\", \"Cascade\", \"Bow Valley\"]\n",
    "wolves_data = wolves_data[wolves_data['Pack name'].isin(packs)]\n",
    "\n",
    "# Update the 'Pack name' column for observations with individual-id B065\n",
    "wolves_data.loc[wolves_data['individual-id'] == 'B065', 'Pack name'] = 'Cascade'\n",
    "\n",
    "# Rearrange the columns to match the elk DataFrame\n",
    "wolves_data = wolves_data[['individual-id', 'event-id', 'timestamp', 'location-long', 'location-lat', 'animal-type', 'Pack name']]\n",
    "\n",
    "# Save the cleaned data\n",
    "output_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advanced Data Analysis/ADA_Project/Data/clean_data_wolf.csv'\n",
    "wolves_data.to_csv(output_path, index=False)\n",
    "\n",
    "# Display the head to verify\n",
    "print(wolves_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning - Elk data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Ya Ha Tinda elk project, Banff National Park, 2001-2023 (females).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_6519/875017485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Data/Ya Ha Tinda elk project, Banff National Park, 2001-2023 (females).csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0melk_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert any timestamps to datetime format if they're not already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Ya Ha Tinda elk project, Banff National Park, 2001-2023 (females).csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data/Ya Ha Tinda elk project, Banff National Park, 2001-2023 (females).csv'\n",
    "elk_data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert any timestamps to datetime format if they're not already\n",
    "elk_data['timestamp'] = pd.to_datetime(elk_data['timestamp'])\n",
    "\n",
    "# Add a new column 'animal-type' with the value 'Elk' for all rows\n",
    "elk_data['animal-type'] = 'Elk'\n",
    "\n",
    "# Rename the individual ID column\n",
    "elk_data['individual-id'] = elk_data['individual-local-identifier']\n",
    "\n",
    "# Filter the DataFrame to include only data from 2001 to 2011\n",
    "elk_data = elk_data[elk_data['timestamp'].dt.year.between(2001, 2011)]\n",
    "\n",
    "# Now create the cleaned DataFrame with only the necessary columns\n",
    "elk_data = elk_data[['individual-id','event-id', 'timestamp', 'location-long', 'location-lat', 'animal-type']].copy()\n",
    "\n",
    "# Save the cleaned data\n",
    "output_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/clean_data_elk.csv'\n",
    "elk_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(elk_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_6519/3342578181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mabsolute_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrelative_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"src/lib\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsolute_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "absolute_path = os.path.dirname(__file__)\n",
    "relative_path = \"src/lib\"\n",
    "full_path = os.path.join(absolute_path, relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge clean_data_elk and clean_data_wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       individual-id    event-id           timestamp  location-long  \\\n",
      "201264          JW02  2048277689 2011-03-16 12:00:25    -115.723292   \n",
      "201265          JW02  2048277690 2011-03-16 14:01:11    -115.724644   \n",
      "201266          JW02  2048277691 2011-03-16 16:02:57    -115.740961   \n",
      "201267          JW02  2048277692 2011-03-16 18:01:26    -115.743783   \n",
      "201268          JW02  2048277693 2011-03-16 20:01:26    -115.743720   \n",
      "\n",
      "        location-lat animal-type   Pack name  \n",
      "201264     51.178754        Wolf  Bow Valley  \n",
      "201265     51.178865        Wolf  Bow Valley  \n",
      "201266     51.185789        Wolf  Bow Valley  \n",
      "201267     51.192357        Wolf  Bow Valley  \n",
      "201268     51.192093        Wolf  Bow Valley  \n"
     ]
    }
   ],
   "source": [
    "# Add a 'year_month' column to both datasets\n",
    "wolves_data['year_month'] = wolves_data['timestamp'].dt.to_period('M')\n",
    "elk_data['year_month'] = elk_data['timestamp'].dt.to_period('M')\n",
    "\n",
    "# Get the unique year-month periods from wolf data\n",
    "wolf_year_months = set(wolves_data['year_month'].unique())\n",
    "\n",
    "# Filter the elk data to only include observations that match the wolf year-month periods\n",
    "elk_data = elk_data[elk_data['year_month'].isin(wolf_year_months)]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_data = pd.concat([elk_data, wolves_data], ignore_index=True)\n",
    "\n",
    "# If you no longer need the 'year_month' column, you can drop it before saving\n",
    "combined_data = combined_data.drop(columns=['year_month'])\n",
    "\n",
    "# Save the combined data\n",
    "combined_data_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv'\n",
    "combined_data.to_csv(combined_data_path, index=False)\n",
    "\n",
    "print(combined_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance travelled between each observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_2216/3134136675.py:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the timestamp column is a datetime object and sort by 'individual-id' and 'timestamp'\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.sort_values(by=['individual-id', 'timestamp'], inplace=True)\n",
    "\n",
    "# Filter out the wolf data\n",
    "wolf_data = data[data['animal-type'] == 'Wolf'].copy()\n",
    "\n",
    "# Initialize an empty list to store distances\n",
    "distances = []\n",
    "\n",
    "# Loop through the DataFrame by each wolf\n",
    "for wolf_id in wolf_data['individual-id'].unique():\n",
    "    # Get the specific wolf's data\n",
    "    wolf_observations = wolf_data[wolf_data['individual-id'] == wolf_id].sort_values('timestamp')\n",
    "    \n",
    "    # Start with a distance of 0.0 for the first observation of each wolf\n",
    "    individual_distances = [0.0]\n",
    "    \n",
    "    # Compute the distance between each consecutive observation\n",
    "    for i in range(1, len(wolf_observations)):\n",
    "        # Get the current and previous observation's location\n",
    "        current_location = (wolf_observations.iloc[i]['location-lat'], wolf_observations.iloc[i]['location-long'])\n",
    "        previous_location = (wolf_observations.iloc[i - 1]['location-lat'], wolf_observations.iloc[i - 1]['location-long'])\n",
    "        \n",
    "        # Calculate the distance and append to the individual_distances list\n",
    "        distance = geodesic(previous_location, current_location).kilometers\n",
    "        individual_distances.append(distance)\n",
    "\n",
    "    # Extend the main distances list with the individual distances\n",
    "    distances.extend(individual_distances)\n",
    "\n",
    "# Add the distances as a new column to the wolf_data DataFrame\n",
    "wolf_data['distance_traveled'] = distances\n",
    "\n",
    "# merge this information back into the main DataFrame\n",
    "data = data.drop(columns=['distance_traveled'], errors='ignore')  # Drop the column if it already exists\n",
    "data = data.merge(wolf_data[['individual-id', 'timestamp', 'distance_traveled']], on=['individual-id', 'timestamp'], how='left')\n",
    "\n",
    "data.to_csv('/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add proximity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_2216/1515831488.py:8: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Separate elk and wolf data based on 'animal-type'\n",
    "elk_data = data[data['animal-type'] == 'Elk']\n",
    "wolf_data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Perform K-Means clustering on elk data\n",
    "elk_coords = elk_data[['location-lat', 'location-long']]\n",
    "kmeans = KMeans(n_clusters=11, random_state=0).fit(elk_coords)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Initialize list to store proximity data\n",
    "proximity_values = []\n",
    "\n",
    "# Calculate proximity for each wolf observation\n",
    "for index, wolf in wolf_data.iterrows():\n",
    "    nearest_distance = float('inf')\n",
    "    for centroid in centroids:\n",
    "        distance = geodesic((wolf['location-lat'], wolf['location-long']), centroid).kilometers\n",
    "        if distance < nearest_distance:\n",
    "            nearest_distance = distance\n",
    "\n",
    "    # Calculate proximity metric and store it with index to ensure alignment\n",
    "    proximity_value = 1 / (nearest_distance + 1)  # Inverse of distance to model closeness\n",
    "    proximity_values.append({'index': index, 'Proximity': proximity_value})\n",
    "\n",
    "# Create a DataFrame from the proximity values\n",
    "proximity_df = pd.DataFrame(proximity_values).set_index('index')\n",
    "\n",
    "# Join the proximity data back to the wolf data using the index\n",
    "wolf_data = wolf_data.join(proximity_df, on=wolf_data.index, rsuffix='_proximity')\n",
    "\n",
    "# Update the main data DataFrame with the new proximity values for wolves\n",
    "data.loc[wolf_data.index, 'Proximity'] = wolf_data['Proximity']\n",
    "\n",
    "# Save the updated DataFrame\n",
    "data.to_csv('/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set index as int \n",
    "Indexing was altered in the merging process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_2216/2574470101.py:5: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/Users/robin/Library/Mobile Documents/com~apple~CloudDocs/UNIL/HEC cours/Master/MA 2/Advandced Data Analysis/Project/Data/merged_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'event-id' column to integer type\n",
    "data['event-id'] = data['event-id'].astype(int)\n",
    "\n",
    "# Optionally, save the DataFrame if you need to preserve changes\n",
    "data.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
