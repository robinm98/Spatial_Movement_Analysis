{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/f_ps01dx04n4r8lqhy1tjk5r0000gn/T/ipykernel_1722/2755767015.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../Data/merged_data.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wolf B042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Data/merged_data.csv')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Define grid search parameters\n",
    "learning_rates = [1e-4] # use only one low learning rate for LSTM, GRU and LSTM+ to avoid exploding gradients (with higher lr the model would output infinite or nan values)\n",
    "nodes = [64, 128] # number of nodes from 64 to 128 for LSTM, GRU and LSTM+, because of computational time and to avoid too complex models\n",
    "layers = [3, 5] # number of layers from 3 to 5 for LSTM, GRU and LSTM+, because of computational time and to avoid too complex models\n",
    "batch_size = 32\n",
    "epochs = 10 #Â due to computational time, I can only train for 10 epochs and 32 batch size (running on a CPU and this code would take hours to run on my machine with higher epochs)\n",
    "patience = 3\n",
    "time_steps = 20 # different lags have been tested from 5 to 40 and 20 seems to be the best tradeoff between accuracy and avoiding overfitting\n",
    "\n",
    "# Prepare a dictionary to store the results for each wolf\n",
    "results = {}\n",
    "\n",
    "# Function to create overlapping windows\n",
    "def create_overlapping_windows(data, time_steps):\n",
    "    X, Y = [], [] \n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        Y.append(data.iloc[i+time_steps].values)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Process each wolf's data\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "    print(f\"Processing wolf {wolf_id}\")\n",
    "    \n",
    "    # Create overlapping windows\n",
    "    X, y = create_overlapping_windows(group[['location-lat', 'location-long']], time_steps)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler_X.fit_transform(X.reshape(-1, 2)).reshape(X.shape)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    n_train = int(0.8 * len(X_scaled))\n",
    "    X_train_scaled, y_train_scaled = X_scaled[:n_train], y_scaled[:n_train]\n",
    "    X_test_scaled, y_test_scaled = X_scaled[n_train:], y_scaled[n_train:]\n",
    "\n",
    "    # Initialize the best score to a high value\n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    # Grid search for hyperparameters\n",
    "    for lr in learning_rates:\n",
    "        for node in nodes:\n",
    "            for layer in layers:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                \n",
    "                model.add(LSTM(node, return_sequences=True, input_shape=(time_steps, 2), activation='relu')) # First LSTM layer\n",
    "                \n",
    "                for _ in range(layer - 2): # -2 because first and last layers are already added\n",
    "                    model.add(LSTM(node, return_sequences=True, activation='relu'))\n",
    "                model.add(LSTM(node, activation='relu'))  # Last LSTM layer\n",
    "                \n",
    "                model.add(Dense(2, activation='linear'))  # Output layer\n",
    "                \n",
    "                # Compile the model\n",
    "                optimizer = Adam(learning_rate=lr, clipvalue=0.1)\n",
    "                model.compile(loss='mse', optimizer=optimizer)\n",
    "                \n",
    "                # Fit the model with early stopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                history = model.fit(\n",
    "                    X_train_scaled, y_train_scaled, \n",
    "                    validation_split=0.2,  # Using 20% of the training data for validation\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate the model on the test data\n",
    "                score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "                print(f\"Model with LR: {lr}, Nodes: {node}, Layers: {layer} has MSE: {score}\")\n",
    "\n",
    "                # Update best model if the score improved\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'lr': lr, 'nodes': node, 'layers': layer}\n",
    "                    best_model = model\n",
    "\n",
    "    # Forecast future values using the last available input from training\n",
    "    current_input = X_train_scaled[-1].reshape(1, time_steps, 2)\n",
    "    predictions = []\n",
    "    for _ in range(len(y_test_scaled)):\n",
    "        next_prediction = best_model.predict(current_input)\n",
    "        predictions.append(next_prediction[0])\n",
    "        current_input = np.roll(current_input, -1, axis=1)\n",
    "        current_input[0, -1, :] = next_prediction[0]\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    predictions_scaled_back = scaler_y.inverse_transform(predictions)\n",
    "    mse = mean_squared_error(scaler_y.inverse_transform(y_test_scaled), predictions_scaled_back)\n",
    "    \n",
    "    results[wolf_id] = {\n",
    "        'mse': mse,\n",
    "        'predictions': predictions_scaled_back,\n",
    "        'actual': scaler_y.inverse_transform(y_test_scaled),\n",
    "        'best_params': best_params\n",
    "    }\n",
    "    \n",
    "    # Save the best model for each wolf\n",
    "    best_model.save(f'../Results/LSTM results/best_model_for_wolf_{wolf_id}.keras')\n",
    "\n",
    "# Output the results for each wolf\n",
    "for wolf_id, res in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}, Test set MSE: {res['mse']}, Best Params: {res['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a map for a single wolf\n",
    "def create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id):\n",
    "    # Starting point for the map\n",
    "    start_lat = wolf_data['location-lat'].iloc[0]\n",
    "    start_long = wolf_data['location-long'].iloc[0]\n",
    "    wolf_map = folium.Map(location=[start_lat, start_long], zoom_start=8)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(wolf_data) * 0.8)\n",
    "    train_data = wolf_data.iloc[:split_index]\n",
    "    test_data = wolf_data.iloc[split_index:]\n",
    "\n",
    "    # Define the HTML code for a purple cross\n",
    "    html_purple_cross = '''\n",
    "    <div style=\"position: relative; width: 8px; height: 8px;\">\n",
    "        <div style=\"position: absolute; top: 50%; left: 0; transform: translate(0%, -50%); width: 100%; height: 2px; background-color: purple;\"></div>\n",
    "        <div style=\"position: absolute; top: 0; left: 50%; transform: translate(-50%, 0%); width: 2px; height: 100%; background-color: purple;\"></div>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "    # Add training data points\n",
    "    train_points = [(row['location-lat'], row['location-long']) for idx, row in train_data.iterrows()]\n",
    "    folium.PolyLine(train_points, color=\"blue\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in train_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            popup='Train'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Add test data points\n",
    "    test_points = [(row['location-lat'], row['location-long']) for idx, row in test_data.iterrows()]\n",
    "    folium.PolyLine(test_points, color=\"green\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in test_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='green',\n",
    "            popup='Test'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Mark the first test observation with a purple cross\n",
    "    folium.Marker(\n",
    "        location=test_points[0],\n",
    "        icon=folium.DivIcon(html=html_purple_cross),\n",
    "        popup='First Test Observation'\n",
    "    ).add_to(wolf_map)\n",
    "\n",
    "    # Add prediction points and draw lines between them\n",
    "    prediction_points = list(zip(test_preds_lat, test_preds_long))\n",
    "    folium.PolyLine(prediction_points, color=\"red\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for idx, point in enumerate(prediction_points):\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            popup=f'Predicted: {idx}'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Save the map in a specific directory for LSTM results\n",
    "    wolf_map.save(f'../Visualisation/LSTM/wolf_{wolf_id}_map.html')\n",
    "\n",
    "# Iterate through each wolf and create a map\n",
    "for wolf_id, info in results.items():\n",
    "    test_preds_lat = info['predictions'][:, 0]  \n",
    "    test_preds_long = info['predictions'][:, 1]\n",
    "    wolf_data = data[data['individual-id'] == wolf_id]\n",
    "    create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM accuracy for each wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - Wolf ID: B042\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1890\n",
      "RMSE Latitude: 0.1914\n",
      "MAE Longitude: 0.3559\n",
      "RMSE Longitude: 0.3613\n",
      "Mean Haversine Distance: 32.55 km\n",
      "Median Haversine Distance: 31.91 km\n",
      "----------\n",
      "LSTM - Wolf ID: B045\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1768\n",
      "RMSE Latitude: 0.1889\n",
      "MAE Longitude: 0.1468\n",
      "RMSE Longitude: 0.1703\n",
      "Mean Haversine Distance: 22.71 km\n",
      "Median Haversine Distance: 23.37 km\n",
      "----------\n",
      "LSTM - Wolf ID: B065\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0967\n",
      "RMSE Latitude: 0.1071\n",
      "MAE Longitude: 0.3682\n",
      "RMSE Longitude: 0.3773\n",
      "Mean Haversine Distance: 27.92 km\n",
      "Median Haversine Distance: 29.12 km\n",
      "----------\n",
      "LSTM - Wolf ID: B077\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0564\n",
      "RMSE Latitude: 0.0756\n",
      "MAE Longitude: 0.3024\n",
      "RMSE Longitude: 0.3261\n",
      "Mean Haversine Distance: 23.38 km\n",
      "Median Haversine Distance: 23.95 km\n",
      "----------\n",
      "LSTM - Wolf ID: B078\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0317\n",
      "RMSE Latitude: 0.0478\n",
      "MAE Longitude: 0.1008\n",
      "RMSE Longitude: 0.1370\n",
      "Mean Haversine Distance: 8.28 km\n",
      "Median Haversine Distance: 5.28 km\n",
      "----------\n",
      "LSTM - Wolf ID: B079\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0644\n",
      "RMSE Latitude: 0.0696\n",
      "MAE Longitude: 0.2024\n",
      "RMSE Longitude: 0.2072\n",
      "Mean Haversine Distance: 16.16 km\n",
      "Median Haversine Distance: 16.12 km\n",
      "----------\n",
      "LSTM - Wolf ID: B080\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1903\n",
      "RMSE Latitude: 0.2090\n",
      "MAE Longitude: 0.1604\n",
      "RMSE Longitude: 0.1783\n",
      "Mean Haversine Distance: 24.59 km\n",
      "Median Haversine Distance: 25.36 km\n",
      "----------\n",
      "LSTM - Wolf ID: B081\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0518\n",
      "RMSE Latitude: 0.0726\n",
      "MAE Longitude: 0.1208\n",
      "RMSE Longitude: 0.1619\n",
      "Mean Haversine Distance: 10.83 km\n",
      "Median Haversine Distance: 7.53 km\n",
      "----------\n",
      "LSTM - Wolf ID: B082\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 5\n",
      "MAE Latitude: 0.0492\n",
      "RMSE Latitude: 0.0557\n",
      "MAE Longitude: 0.1810\n",
      "RMSE Longitude: 0.1973\n",
      "Mean Haversine Distance: 14.09 km\n",
      "Median Haversine Distance: 13.81 km\n",
      "----------\n",
      "LSTM - Wolf ID: B083\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0625\n",
      "RMSE Latitude: 0.0794\n",
      "MAE Longitude: 0.0507\n",
      "RMSE Longitude: 0.0669\n",
      "Mean Haversine Distance: 8.61 km\n",
      "Median Haversine Distance: 7.53 km\n",
      "----------\n",
      "LSTM - Wolf ID: B084\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1067\n",
      "RMSE Latitude: 0.1218\n",
      "MAE Longitude: 0.2679\n",
      "RMSE Longitude: 0.2997\n",
      "Mean Haversine Distance: 22.47 km\n",
      "Median Haversine Distance: 23.51 km\n",
      "----------\n",
      "LSTM - Wolf ID: B085\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1143\n",
      "RMSE Latitude: 0.1278\n",
      "MAE Longitude: 0.1050\n",
      "RMSE Longitude: 0.1297\n",
      "Mean Haversine Distance: 15.19 km\n",
      "Median Haversine Distance: 14.51 km\n",
      "----------\n",
      "LSTM - Wolf ID: B086\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0332\n",
      "RMSE Latitude: 0.0419\n",
      "MAE Longitude: 0.3138\n",
      "RMSE Longitude: 0.3316\n",
      "Mean Haversine Distance: 22.40 km\n",
      "Median Haversine Distance: 22.52 km\n",
      "----------\n",
      "LSTM - Wolf ID: B087\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.0690\n",
      "RMSE Latitude: 0.0881\n",
      "MAE Longitude: 0.1867\n",
      "RMSE Longitude: 0.2242\n",
      "Mean Haversine Distance: 16.56 km\n",
      "Median Haversine Distance: 17.26 km\n",
      "----------\n",
      "LSTM - Wolf ID: JW01\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.5122\n",
      "RMSE Latitude: 0.5186\n",
      "MAE Longitude: 0.9734\n",
      "RMSE Longitude: 0.9760\n",
      "Mean Haversine Distance: 89.13 km\n",
      "Median Haversine Distance: 91.15 km\n",
      "----------\n",
      "LSTM - Wolf ID: JW02\n",
      "Best Model: Learning Rate: 1e-05, Nodes: 128, Layers: 3\n",
      "MAE Latitude: 0.1652\n",
      "RMSE Latitude: 0.1983\n",
      "MAE Longitude: 0.3248\n",
      "RMSE Longitude: 0.3528\n",
      "Mean Haversine Distance: 32.78 km\n",
      "Median Haversine Distance: 30.80 km\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points \n",
    "    on the Earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers. Adjust if using a different radius\n",
    "    return c * r\n",
    "\n",
    "for wolf_id, res in results.items():\n",
    "    test_data = res['actual']\n",
    "    predictions = res['predictions']\n",
    "    \n",
    "    # Extract latitude and longitude from predictions and actual data\n",
    "    lat_preds, lon_preds = predictions[:, 0], predictions[:, 1]\n",
    "    lat_actual, lon_actual = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "    # Calculate the Haversine distance for each prediction\n",
    "    distances = [haversine(lon_p, lat_p, lon_a, lat_a) for lon_p, lat_p, lon_a, lat_a in zip(lon_preds, lat_preds, lon_actual, lat_actual)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae_lat = mean_absolute_error(lat_actual, lat_preds)\n",
    "    rmse_lat = sqrt(mean_squared_error(lat_actual, lat_preds))\n",
    "    mae_lon = mean_absolute_error(lon_actual, lon_preds)\n",
    "    rmse_lon = sqrt(mean_squared_error(lon_actual, lon_preds))\n",
    "    \n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    \n",
    "    # Store extended metrics in the results dictionary\n",
    "    res.update({\n",
    "        'MAE_Latitude': mae_lat,\n",
    "        'RMSE_Latitude': rmse_lat,\n",
    "        'MAE_Longitude': mae_lon,\n",
    "        'RMSE_Longitude': rmse_lon,\n",
    "        'Mean_Haversine_Distance': mean_distance,\n",
    "        'Median_Haversine_Distance': median_distance\n",
    "    })\n",
    "\n",
    "# Print the metrics for each wolf\n",
    "for wolf_id, info in results.items():\n",
    "    print(f\"LSTM - Wolf ID: {wolf_id}\")\n",
    "    print(f\"Best Model: Learning Rate: {info['best_params']['lr']}, Nodes: {info['best_params']['nodes']}, Layers: {info['best_params']['layers']}\")\n",
    "    print(f\"MAE Latitude: {info['MAE_Latitude']:.4f}\")\n",
    "    print(f\"RMSE Latitude: {info['RMSE_Latitude']:.4f}\")\n",
    "    print(f\"MAE Longitude: {info['MAE_Longitude']:.4f}\")\n",
    "    print(f\"RMSE Longitude: {info['RMSE_Longitude']:.4f}\")\n",
    "    print(f\"Mean Haversine Distance: {info['Mean_Haversine_Distance']:.2f} km\")\n",
    "    print(f\"Median Haversine Distance: {info['Median_Haversine_Distance']:.2f} km\")\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM - Average Metrics for All Wolves:\n",
      "Average MAE Latitude: 0.1231\n",
      "Average RMSE Latitude: 0.1371\n",
      "Average MAE Longitude: 0.2601\n",
      "Average RMSE Longitude: 0.2811\n",
      "Average Mean Haversine Distance: 24.23 km\n",
      "Average Median Haversine Distance: 23.98 km\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to accumulate the metrics\n",
    "total_mae_lat = 0\n",
    "total_rmse_lat = 0\n",
    "total_mae_long = 0\n",
    "total_rmse_long = 0\n",
    "total_mean_haversine_distance = 0\n",
    "total_median_haversine_distance = 0\n",
    "num_wolves = len(results)\n",
    "\n",
    "# Loop over each wolf's results\n",
    "for wolf_id, info in results.items():\n",
    "    # Accumulate individual metrics\n",
    "    total_mae_lat += info['MAE_Latitude']\n",
    "    total_rmse_lat += info['RMSE_Latitude']\n",
    "    total_mae_long += info['MAE_Longitude']\n",
    "    total_rmse_long += info['RMSE_Longitude']\n",
    "    total_mean_haversine_distance += info['Mean_Haversine_Distance']\n",
    "    total_median_haversine_distance += info['Median_Haversine_Distance']\n",
    "\n",
    "# Calculate average metrics\n",
    "average_mae_lat = total_mae_lat / num_wolves\n",
    "average_rmse_lat = total_rmse_lat / num_wolves\n",
    "average_mae_long = total_mae_long / num_wolves\n",
    "average_rmse_long = total_rmse_long / num_wolves\n",
    "average_mean_haversine_distance = total_mean_haversine_distance / num_wolves\n",
    "average_median_haversine_distance = total_median_haversine_distance / num_wolves\n",
    "\n",
    "# Print average metrics\n",
    "print(\"LSTM - Average Metrics for All Wolves:\")\n",
    "print(f\"Average MAE Latitude: {average_mae_lat:.4f}\")\n",
    "print(f\"Average RMSE Latitude: {average_rmse_lat:.4f}\")\n",
    "print(f\"Average MAE Longitude: {average_mae_long:.4f}\")\n",
    "print(f\"Average RMSE Longitude: {average_rmse_long:.4f}\")\n",
    "print(f\"Average Mean Haversine Distance: {average_mean_haversine_distance:.2f} km\")\n",
    "print(f\"Average Median Haversine Distance: {average_median_haversine_distance:.2f} km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "\n",
    "    # Convert results dictionary to a DataFrame for easier CSV saving\n",
    "    results_df = pd.DataFrame([results[wolf_id]])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_csv_path = f'../Results/LSTM results/results_{wolf_id}.csv'\n",
    "    results_df.to_csv(results_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
