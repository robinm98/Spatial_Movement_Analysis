{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA Project\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = pd.read_csv('../Data/merged_data.csv')\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data[data['animal-type'] == 'Wolf']\n",
    "\n",
    "# Define grid search parameters\n",
    "learning_rates = [1e-5, 1e-4, 1e-3]\n",
    "nodes = [32, 64, 128, 256] # high number of nodes because all best models tested have between 64 to 256 nodes\n",
    "layers = [3, 5, 7] # only 3, 5, 7 to avoid too complex models and overfitting\n",
    "batch_size = 32\n",
    "epochs = 10 #Â due to computational time, I can only train for 10 epochs and 32 batch size (running on a CPU and this code takes already hours to run on my machine)\n",
    "patience = 3\n",
    "time_steps = 20 # different lags have been tested and 20 seems to be the best tradeoff between accuracy and avoiding overfitting\n",
    "\n",
    "# Prepare a dictionary to store the results for each wolf\n",
    "results = {}\n",
    "\n",
    "# Function to create overlapping windows\n",
    "def create_overlapping_windows(data, time_steps):\n",
    "    X, Y = [], [] \n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        Y.append(data.iloc[i+time_steps].values)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Process each wolf's data\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "    print(f\"Processing wolf {wolf_id}\")\n",
    "    \n",
    "    # Create overlapping windows\n",
    "    X, y = create_overlapping_windows(group[['location-lat', 'location-long']], time_steps)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_scaled = scaler_X.fit_transform(X.reshape(-1, 2)).reshape(X.shape)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "    \n",
    "    # Split the data into training and testing\n",
    "    n_train = int(0.8 * len(X_scaled))\n",
    "    X_train_scaled, y_train_scaled = X_scaled[:n_train], y_scaled[:n_train]\n",
    "    X_test_scaled, y_test_scaled = X_scaled[n_train:], y_scaled[n_train:]\n",
    "\n",
    "    # Initialize the best score to a high value\n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "\n",
    "    # Grid search for hyperparameters\n",
    "    for lr in learning_rates:\n",
    "        for node in nodes:\n",
    "            for layer in layers:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                \n",
    "                model.add(LSTM(node, return_sequences=True, input_shape=(time_steps, 2), activation='relu')) # First LSTM layer\n",
    "                \n",
    "                for _ in range(layer - 2): # -2 because first and last layers are already added\n",
    "                    model.add(LSTM(node, return_sequences=True, activation='relu'))\n",
    "                model.add(LSTM(node, activation='relu'))  # Last LSTM layer\n",
    "                \n",
    "                model.add(Dense(2, activation='linear'))  # Output layer\n",
    "                \n",
    "                # Compile the model\n",
    "                optimizer = Adam(learning_rate=lr)\n",
    "                model.compile(loss='mse', optimizer=optimizer)\n",
    "                \n",
    "                # Fit the model with early stopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "                history = model.fit(\n",
    "                    X_train_scaled, y_train_scaled, \n",
    "                    validation_split=0.2,  # Using 20% of the training data for validation\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stopping], \n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Evaluate the model on the test data\n",
    "                score = model.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "                print(f\"Model with LR: {lr}, Nodes: {node}, Layers: {layer} has MSE: {score}\")\n",
    "\n",
    "                # Update best model if the score improved\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'lr': lr, 'nodes': node, 'layers': layer}\n",
    "                    best_model = model\n",
    "\n",
    "    # Forecast future values using the last available input from training\n",
    "    current_input = X_train_scaled[-1].reshape(1, time_steps, 2)\n",
    "    predictions = []\n",
    "    for _ in range(len(y_test_scaled)):\n",
    "        next_prediction = best_model.predict(current_input)\n",
    "        predictions.append(next_prediction[0])\n",
    "        current_input = np.roll(current_input, -1, axis=1)\n",
    "        current_input[0, -1, :] = next_prediction[0]\n",
    "\n",
    "    # Inverse transform predictions to original scale\n",
    "    predictions_scaled_back = scaler_y.inverse_transform(predictions)\n",
    "    mse = mean_squared_error(scaler_y.inverse_transform(y_test_scaled), predictions_scaled_back)\n",
    "    \n",
    "    results[wolf_id] = {\n",
    "        'mse': mse,\n",
    "        'predictions': predictions_scaled_back,\n",
    "        'actual': scaler_y.inverse_transform(y_test_scaled),\n",
    "        'best_params': best_params\n",
    "    }\n",
    "    \n",
    "    # Save the best model for each wolf\n",
    "    best_model.save(f'../Results/LSTM results/best_model_for_wolf_{wolf_id}.keras')\n",
    "\n",
    "# Output the results for each wolf\n",
    "for wolf_id, res in results.items():\n",
    "    print(f\"Wolf ID: {wolf_id}, Test set MSE: {res['mse']}, Best Params: {res['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create a map for a single wolf\n",
    "def create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id):\n",
    "    # Starting point for the map\n",
    "    start_lat = wolf_data['location-lat'].iloc[0]\n",
    "    start_long = wolf_data['location-long'].iloc[0]\n",
    "    wolf_map = folium.Map(location=[start_lat, start_long], zoom_start=8)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(wolf_data) * 0.8)\n",
    "    train_data = wolf_data.iloc[:split_index]\n",
    "    test_data = wolf_data.iloc[split_index:]\n",
    "\n",
    "    # Define the HTML code for a purple cross\n",
    "    html_purple_cross = '''\n",
    "    <div style=\"position: relative; width: 8px; height: 8px;\">\n",
    "        <div style=\"position: absolute; top: 50%; left: 0; transform: translate(0%, -50%); width: 100%; height: 2px; background-color: purple;\"></div>\n",
    "        <div style=\"position: absolute; top: 0; left: 50%; transform: translate(-50%, 0%); width: 2px; height: 100%; background-color: purple;\"></div>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "    # Add training data points\n",
    "    train_points = [(row['location-lat'], row['location-long']) for idx, row in train_data.iterrows()]\n",
    "    folium.PolyLine(train_points, color=\"blue\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in train_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_color='blue',\n",
    "            popup='Train'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Add test data points\n",
    "    test_points = [(row['location-lat'], row['location-long']) for idx, row in test_data.iterrows()]\n",
    "    folium.PolyLine(test_points, color=\"green\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for point in test_points:\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_color='green',\n",
    "            popup='Test'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Mark the first test observation with a purple cross\n",
    "    folium.Marker(\n",
    "        location=test_points[0],\n",
    "        icon=folium.DivIcon(html=html_purple_cross),\n",
    "        popup='First Test Observation'\n",
    "    ).add_to(wolf_map)\n",
    "\n",
    "    # Add prediction points and draw lines between them\n",
    "    prediction_points = list(zip(test_preds_lat, test_preds_long))\n",
    "    folium.PolyLine(prediction_points, color=\"red\", weight=2.5, opacity=1).add_to(wolf_map)\n",
    "    for idx, point in enumerate(prediction_points):\n",
    "        folium.CircleMarker(\n",
    "            location=point,\n",
    "            radius=3,\n",
    "            color='red',\n",
    "            fill=True,\n",
    "            fill_color='red',\n",
    "            popup=f'Predicted: {idx}'\n",
    "        ).add_to(wolf_map)\n",
    "\n",
    "    # Save the map in a specific directory for FNN results\n",
    "    wolf_map.save(f'../Visualisation/LSTM/wolf_{wolf_id}_map.html')\n",
    "\n",
    "# Iterate through each wolf and create a map\n",
    "for wolf_id, info in results.items():\n",
    "    test_preds_lat = info['predictions'][:, 0]  \n",
    "    test_preds_long = info['predictions'][:, 1]\n",
    "    wolf_data = data[data['individual-id'] == wolf_id]\n",
    "    create_wolf_map(wolf_data, test_preds_lat, test_preds_long, wolf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM accuracy for each wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points \n",
    "    on the Earth (specified in decimal degrees).\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371  # Radius of Earth in kilometers. Adjust if using a different radius\n",
    "    return c * r\n",
    "\n",
    "for wolf_id, res in results.items():\n",
    "    test_data = res['actual']\n",
    "    predictions = res['predictions']\n",
    "    \n",
    "    # Extract latitude and longitude from predictions and actual data\n",
    "    lat_preds, lon_preds = predictions[:, 0], predictions[:, 1]\n",
    "    lat_actual, lon_actual = test_data[:, 0], test_data[:, 1]\n",
    "\n",
    "    # Calculate the Haversine distance for each prediction\n",
    "    distances = [haversine(lon_p, lat_p, lon_a, lat_a) for lon_p, lat_p, lon_a, lat_a in zip(lon_preds, lat_preds, lon_actual, lat_actual)]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae_lat = mean_absolute_error(lat_actual, lat_preds)\n",
    "    rmse_lat = sqrt(mean_squared_error(lat_actual, lat_preds))\n",
    "    mae_lon = mean_absolute_error(lon_actual, lon_preds)\n",
    "    rmse_lon = sqrt(mean_squared_error(lon_actual, lon_preds))\n",
    "    \n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    \n",
    "    # Store extended metrics in the results dictionary\n",
    "    res.update({\n",
    "        'MAE_Latitude': mae_lat,\n",
    "        'RMSE_Latitude': rmse_lat,\n",
    "        'MAE_Longitude': mae_lon,\n",
    "        'RMSE_Longitude': rmse_lon,\n",
    "        'Mean_Haversine_Distance': mean_distance,\n",
    "        'Median_Haversine_Distance': median_distance\n",
    "    })\n",
    "\n",
    "# Print the metrics for each wolf\n",
    "for wolf_id, info in results.items():\n",
    "    print(f\"LSTM - Wolf ID: {wolf_id}\")\n",
    "    print(f\"Best Model: Learning Rate: {info['best_params']['lr']}, Nodes: {info['best_params']['nodes']}, Layers: {info['best_params']['layers']}\")\n",
    "    print(f\"MAE Latitude: {info['MAE_Latitude']:.4f}\")\n",
    "    print(f\"RMSE Latitude: {info['RMSE_Latitude']:.4f}\")\n",
    "    print(f\"MAE Longitude: {info['MAE_Longitude']:.4f}\")\n",
    "    print(f\"RMSE Longitude: {info['RMSE_Longitude']:.4f}\")\n",
    "    print(f\"Mean Haversine Distance: {info['Mean_Haversine_Distance']:.2f} km\")\n",
    "    print(f\"Median Haversine Distance: {info['Median_Haversine_Distance']:.2f} km\")\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate the metrics\n",
    "total_mae_lat = 0\n",
    "total_rmse_lat = 0\n",
    "total_mae_long = 0\n",
    "total_rmse_long = 0\n",
    "total_mean_haversine_distance = 0\n",
    "total_median_haversine_distance = 0\n",
    "num_wolves = len(results)\n",
    "\n",
    "# Loop over each wolf's results, excluding wolf B087\n",
    "for wolf_id, info in results.items():\n",
    "    if wolf_id != 'B087':\n",
    "        # Accumulate individual metrics\n",
    "        total_mae_lat += info['MAE_Latitude']\n",
    "        total_rmse_lat += info['RMSE_Latitude']\n",
    "        total_mae_long += info['MAE_Longitude']\n",
    "        total_rmse_long += info['RMSE_Longitude']\n",
    "        total_mean_haversine_distance += info['Mean_Haversine_Distance']\n",
    "        total_median_haversine_distance += info['Median_Haversine_Distance']\n",
    "\n",
    "# Calculate average metrics excluding wolf B087\n",
    "num_wolves_excluding_B087 = num_wolves - 1  # Subtract 1 for wolf B087\n",
    "average_mae_lat = total_mae_lat / num_wolves_excluding_B087\n",
    "average_rmse_lat = total_rmse_lat / num_wolves_excluding_B087\n",
    "average_mae_long = total_mae_long / num_wolves_excluding_B087\n",
    "average_rmse_long = total_rmse_long / num_wolves_excluding_B087\n",
    "average_mean_haversine_distance = total_mean_haversine_distance / num_wolves_excluding_B087\n",
    "average_median_haversine_distance = total_median_haversine_distance / num_wolves_excluding_B087\n",
    "\n",
    "# Print average metrics excluding wolf B087\n",
    "print(\"LSTM - Average Metrics for All Wolves:\")\n",
    "print(f\"Average MAE Latitude: {average_mae_lat:.4f}\")\n",
    "print(f\"Average RMSE Latitude: {average_rmse_lat:.4f}\")\n",
    "print(f\"Average MAE Longitude: {average_mae_long:.4f}\")\n",
    "print(f\"Average RMSE Longitude: {average_rmse_long:.4f}\")\n",
    "print(f\"Average Mean Haversine Distance: {average_mean_haversine_distance:.2f} km\")\n",
    "print(f\"Average Median Haversine Distance: {average_median_haversine_distance:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for wolf_id, group in data.groupby('individual-id'):\n",
    "\n",
    "    # Convert results dictionary to a DataFrame for easier CSV saving\n",
    "    results_df = pd.DataFrame([results[wolf_id]])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_csv_path = f'../Results/LSTM results/results_{wolf_id}.csv'\n",
    "    results_df.to_csv(results_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
